{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, OrdinalEncoder, MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.feature_selection import f_classif, mutual_info_classif\n",
    "from sklearn.metrics import accuracy_score, mean_squared_error\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "\n",
    "from sklearn.neighbors import KNeighborsRegressor, KNeighborsClassifier\n",
    "from sklearn.linear_model import Ridge, LogisticRegression, LinearRegression, Lasso, ElasticNet\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import precision_score, recall_score, accuracy_score, fbeta_score\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import average_precision_score\n",
    "import shap\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. functions used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the dataset into Other set and Test set\n",
    "def basic_split(X, y, other_size, test_size, random_state):\n",
    "    # test the inputs\n",
    "    if (other_size+test_size != 1):\n",
    "        print(\"The split size does not add up to 1\")\n",
    "        raise ValueError\n",
    "    if not isinstance(random_state, int):\n",
    "        print(\"The random state entered in not an integer\")\n",
    "        raise ValueError\n",
    "    if X.shape[0] != y.shape[0]:\n",
    "        print(\"The number of rows in X is not the same as the length of y\")\n",
    "        raise ValueError\n",
    "    if len(X.shape) != 2:\n",
    "        print(\"Input X is not a 2-D pandas dataframe\")\n",
    "        raise ValueError\n",
    "    if len(y.shape) != 1:\n",
    "        print(\"Input y is not a 1-D pandas series\")\n",
    "        raise ValueError\n",
    "\n",
    "    # split the dataset into other and test\n",
    "    X_other, X_test, y_other, y_test = train_test_split(X, y,\n",
    "                                                          train_size = other_size, \n",
    "                                                          random_state = random_state)\n",
    "  \n",
    "    return X_other, y_other, X_test, y_test\n",
    "\n",
    "# function for the ML pipeline as outlined above \n",
    "def MLpipe_KFold_accuracy(X, y, preprocessor, ML_algo, param_grid):\n",
    "\n",
    "    # lists to be returned \n",
    "    test_scores = []\n",
    "    best_models = []\n",
    "    model = pd.DataFrame(columns=['best_parameter', 'train_score', 'test_score'])\n",
    "\n",
    "    for i in range(5):\n",
    "        print(f'---------Random State = {42*i}---------')\n",
    "        # split the data\n",
    "        X_other, X_test, y_other, y_test = train_test_split(X, y, train_size = 0.8, random_state = 42*i)\n",
    "\n",
    "        # use KFold with 4 folds\n",
    "        kf = KFold(n_splits=4,shuffle=True,random_state=42*i)\n",
    "\n",
    "        pipe = make_pipeline(preprocessor,ML_algo)\n",
    "\n",
    "        # GridSearchCV, loop through all possible parameters\n",
    "        # preprocess data and perform cross valifation\n",
    "        grid = GridSearchCV(pipe, param_grid=param_grid,scoring='accuracy',\n",
    "                        cv=kf, return_train_score = True, n_jobs=-1, verbose=3) # return_train_score = True, n_jobs=-1, verbose=True\n",
    "        \n",
    "        grid.fit(X_other, y_other)\n",
    "        results = pd.DataFrame(grid.cv_results_)\n",
    "\n",
    "        # calculate and save the test score\n",
    "        final_model = grid.best_estimator_\n",
    "\n",
    "        y_train_pred = final_model.predict(X_other)\n",
    "        train_accuracy= accuracy_score(y_other,y_train_pred)\n",
    "\n",
    "        y_test_pred = final_model.predict(X_test)\n",
    "        test_scores.append(accuracy_score(y_test,y_test_pred)) # calculate the rmse\n",
    "        best_models.append(grid.best_params_) # save the best param \n",
    "\n",
    "        model.loc[len(model)] = [grid.best_params_, train_accuracy, accuracy_score(y_test,y_test_pred)]\n",
    "    \n",
    "    print('------------acuuracy scores of each random state------------')\n",
    "    print(model)\n",
    "\n",
    "    return test_scores, best_models, results, model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. before modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('mushroom/secondary_data.csv', sep=';')\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert features\n",
    "df['does-bruise-or-bleed'] = df['does-bruise-or-bleed'].replace('t', 1)\n",
    "df['does-bruise-or-bleed'] = df['does-bruise-or-bleed'].replace('f', 0)\n",
    "df['has-ring'] = df['has-ring'].replace('t', 1)\n",
    "df['has-ring'] = df['has-ring'].replace('f', 0)\n",
    "\n",
    "# split the features and target variable\n",
    "y = df['class']\n",
    "X = df.drop(columns='class')\n",
    "print(X.shape, y.shape)\n",
    "\n",
    "# replace the e with 0, and p with 1\n",
    "y = y.replace('e', 0)\n",
    "y = y.replace('p', 1)\n",
    "\n",
    "# split the dataset\n",
    "X_other, y_other, X_test, y_test = basic_split(X, y, other_size = 0.8, test_size = 0.2, random_state=42)\n",
    "print('The shape of each train, val, and test set are the following:')\n",
    "print('X_other: {a}, X_test: {b}'.format(a=X_other.shape, b=X_test.shape))\n",
    "print('y_other: {a}, y_test: {b}'.format(a=y_other.shape, b=y_test.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Define the Preprocessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define preprocessors\n",
    "binary_feature = ['does-bruise-or-bleed', 'has-ring']\n",
    "onehot_features = ['cap-shape', 'cap-surface', 'cap-color', 'gill-attachment', 'gill-spacing', 'gill-color','stem-root', \n",
    "                'stem-surface', 'stem-color', 'veil-type', 'veil-color', 'ring-type', 'spore-print-color', 'habitat', 'season']\n",
    "std_features = ['cap-diameter','stem-height','stem-width']\n",
    "\n",
    "# collect all the encoders\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('onehot', OneHotEncoder(sparse_output=False,handle_unknown='ignore'), onehot_features),\n",
    "        ('std', StandardScaler(), std_features)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = Pipeline(steps=[('preprocessor', preprocessor)]) # for now we only preprocess \n",
    "                                                       # later on we will add other steps here\n",
    "clf.set_output(transform='pandas')\n",
    "\n",
    "X_other_prep = clf.fit_transform(X_other) # save for later use\n",
    "X_test_prep = clf.transform(X_test)\n",
    "\n",
    "print('Shape of X_train before transformation:', X_other.shape)\n",
    "print('Shape of X_train after transformation:', X_other_prep.shape)\n",
    "print('Note: the X_train here refers to X_other in code')\n",
    "X_other_prep.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Baseline Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict all class as poisonous\n",
    "baseline_pred = np.ones(len(y_test))\n",
    "baseline_acc = accuracy_score(y_test, baseline_pred)\n",
    "\n",
    "print(f\"Baseline Accuracy: {baseline_acc:.5f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a dataframe to store the models' results\n",
    "res_accuracy = pd.DataFrame(columns=['model', 'mean_test_accuracy', 'std_test_accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression\n",
    "param_grid = {\n",
    "    'logisticregression__C': [1/0.001, 1/0.01, 1/0.1, 1/1.0]  # l1\n",
    "}\n",
    "ML_algo = LogisticRegression(random_state=42, max_iter=5000)\n",
    "\n",
    "print(\"___________Model {}___________\".format('Logistic Regression'))\n",
    "test_scores, best_models, res_lr, model_lr = MLpipe_KFold_accuracy(X, y, preprocessor, ML_algo, param_grid)\n",
    "\n",
    "# Print the test scores\n",
    "print(\"Test Scores:\", test_scores)\n",
    "\n",
    "# Calculate and print the mean and standard deviation of the test scores\n",
    "mean_accuracy = np.mean(test_scores)\n",
    "std_accuracy = np.std(test_scores)\n",
    "\n",
    "res_accuracy.loc[len(res_accuracy)] = ['Logistic Regression', mean_accuracy, std_accuracy]\n",
    "\n",
    "print(\"Mean accuracy: {}\".format(mean_accuracy))\n",
    "print(\"Standard Deviation of accuracy: {}\".format(std_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_test_score = pd.DataFrame(columns=['Model', 'mean_test', 'std_test'])\n",
    "res_test_score.loc[len(res_test_score)] = ['Logistic Regression', model_lr['test_score'].mean(), model_lr['test_score'].std()]\n",
    "res_test_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# random forest\n",
    "param_grid = {\n",
    "    'randomforestclassifier__max_depth': [5, 10, 20],  # RF\n",
    "    'randomforestclassifier__max_features': [0.25, 0.5, 0.75],\n",
    "    'randomforestclassifier__n_estimators': [20, 50, 100]\n",
    "}\n",
    "ML_algo = RandomForestClassifier(random_state=42, max_features='auto')\n",
    "\n",
    "print(\"___________Model {}___________\".format('Random Forest'))\n",
    "test_scores, best_models, res_rf, model_rf= MLpipe_KFold_accuracy(X, y, preprocessor, ML_algo, param_grid)\n",
    "\n",
    "# Print the test scores\n",
    "print(\"Test Scores:\", test_scores)\n",
    "\n",
    "# Calculate and print the mean and standard deviation of the test scores\n",
    "mean_accuracy = np.mean(test_scores)\n",
    "std_accuracy = np.std(test_scores)\n",
    "\n",
    "res_accuracy.loc[len(res_accuracy)] = ['Random Forest', mean_accuracy, std_accuracy]\n",
    "\n",
    "print(\"Mean accuracy: {}\".format(mean_accuracy))\n",
    "print(\"Standard Deviation of accuracy: {}\".format(std_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_rf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_test_score.loc[len(res_test_score)] = ['Random Forest', model_rf['test_score'].mean(), model_rf['test_score'].std()]\n",
    "res_test_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 Support vector classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# support vector classification\n",
    "\n",
    "# function for the ML pipeline as outlined above \n",
    "def MLpipe_KFold_accuracy(X, y, preprocessor, ML_algo, param_grid):\n",
    "\n",
    "    # lists to be returned \n",
    "    test_scores = []\n",
    "    best_models = []\n",
    "    model_svc = pd.DataFrame(columns=['best_parameter', 'train_score', 'test_score'])\n",
    "\n",
    "    for i in range(5):\n",
    "        # split the data\n",
    "        X_other, X_test, y_other, y_test = train_test_split(X, y, train_size = 0.8, random_state = 42*i)\n",
    "\n",
    "        # use KFold with 4 folds\n",
    "        kf = KFold(n_splits=4,shuffle=True,random_state=42*i)\n",
    "\n",
    "        pipe = make_pipeline(preprocessor,ML_algo)\n",
    "\n",
    "        # GridSearchCV, loop through all possible parameters\n",
    "        # preprocess data and perform cross valifation\n",
    "        grid = GridSearchCV(pipe, param_grid=param_grid,scoring='accuracy',\n",
    "                        cv=kf, return_train_score = True, n_jobs=-1, verbose=3) # return_train_score = True, n_jobs=-1, verbose=True\n",
    "        \n",
    "        grid.fit(X_other, y_other)\n",
    "        results = pd.DataFrame(grid.cv_results_)\n",
    "\n",
    "        # calculate and save the test score\n",
    "        final_model = grid.best_estimator_\n",
    "        train_accuracy= grid_search.score(X_other, y_other)\n",
    "        y_test_pred = final_model.predict(X_test)\n",
    "        test_scores.append(accuracy_score(y_test,y_test_pred)) # calculate the rmse\n",
    "        best_models.append(grid.best_params_) # save the best param \n",
    "\n",
    "        model_svc.loc[len(model_svc)] = [grid.best_params_, train_accuracy, accuracy_score(y_test,y_test_pred)]\n",
    "\n",
    "    return test_scores, best_models, results, model_svc\n",
    "\n",
    "param_grid = {\n",
    "    'svc__gamma': [1e-2, 1e-1, 1e1, 1e3],  # SVR\n",
    "    'svc__C': [1e-1, 1e0, 1e1, 1e2]\n",
    "}\n",
    "ML_algo = SVC(kernel='poly', class_weight='balanced', random_state=42)\n",
    "\n",
    "print(\"___________Model {}___________\".format('Support Vector Classification'))\n",
    "test_scores, best_models, res_svc, model_svc = MLpipe_KFold_accuracy(X, y, preprocessor, ML_algo, param_grid)\n",
    "\n",
    "# Print the test scores\n",
    "print(\"Test Scores:\", test_scores)\n",
    "\n",
    "# Calculate and print the mean and standard deviation of the test scores\n",
    "mean_accuracy = np.mean(test_scores)\n",
    "std_accuracy = np.std(test_scores)\n",
    "\n",
    "res_accuracy.loc[len(res_accuracy)] = ['Support Vector Classification', mean_accuracy, std_accuracy]\n",
    "\n",
    "print(\"Mean accuracy: {}\".format(mean_accuracy))\n",
    "print(\"Standard Deviation of accuracy: {}\".format(std_accuracy))\n",
    "print(model_svc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_test_score.loc[len(res_test_score)] = ['SVC', np.mean(test_scores), np.std(test_scores)]\n",
    "res_test_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.4 KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# knn\n",
    "param_grid = {\n",
    "    'kneighborsclassifier__n_neighbors': [3,9,12,15,30,50,100]  # knn\n",
    "}\n",
    "ML_algo = KNeighborsClassifier()\n",
    "\n",
    "print(\"___________Model {}___________\".format('knn'))\n",
    "test_scores, best_models, res_knn, model_knn = MLpipe_KFold_accuracy(X, y, preprocessor, ML_algo, param_grid)\n",
    "\n",
    "# Print the test scores\n",
    "print(\"Test Scores:\", test_scores)\n",
    "\n",
    "# Calculate and print the mean and standard deviation of the test scores\n",
    "mean_accuracy = np.mean(test_scores)\n",
    "std_accuracy = np.std(test_scores)\n",
    "\n",
    "res_accuracy.loc[len(res_accuracy)] = ['KNN', mean_accuracy, std_accuracy]\n",
    "\n",
    "print(\"Mean accuracy: {}\".format(mean_accuracy))\n",
    "print(\"Standard Deviation of accuracy: {}\".format(std_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_knn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_test_score.loc[len(res_test_score)] = ['KNN', model_knn['test_score'].mean(), model_knn['test_score'].std()]\n",
    "res_test_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.5 xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# xgboost\n",
    "\n",
    "# Define hyperparameter grid for tuning\n",
    "param_grid = {\n",
    "    'xgbclassifier__max_depth': [3, 5, 7, 10],\n",
    "    'xgbclassifier__min_child_weight': [1, 3, 5],\n",
    "    'xgbclassifier__learning_rate': [0.1],\n",
    "    'xgbclassifier__lambda': [0.01, 0.1, 1],  # reduce overfitting\n",
    "    'xgbclassifier__alpha': [0.01, 0.1, 1]  # Used for high dimensionality\n",
    "}\n",
    "pg = ParameterGrid(param_grid)\n",
    "\n",
    "# Train the pipeline on five different random states\n",
    "random_states = [42, 123, 456, 789, 101]\n",
    "train_scores = []\n",
    "test_scores = []\n",
    "best_models = []\n",
    "test_sets = []\n",
    "\n",
    "for random_state in random_states:\n",
    "    # Split the data into train and test sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=random_state)\n",
    "\n",
    "    # use KFold with 4 folds\n",
    "    kf = KFold(n_splits=4,shuffle=True,random_state=random_state)\n",
    "\n",
    "    # Fit the ColumnTransformer on the training data\n",
    "    X_train_transformed = preprocessor.fit_transform(X_train)\n",
    "\n",
    "    # Create XGBoost model\n",
    "    model = XGBClassifier(random_state=random_state)\n",
    "\n",
    "    # define pipeline\n",
    "    pipeline = make_pipeline(preprocessor, model)\n",
    "\n",
    "    # Create GridSearchCV\n",
    "    grid_search = GridSearchCV(estimator=pipeline, param_grid=param_grid, scoring='accuracy', cv=kf, return_train_score=True, verbose=3)\n",
    "\n",
    "    # Fit the model\n",
    "    grid_search.fit(X_train, y_train, xgbclassifier__eval_metric=\"logloss\", xgbclassifier__early_stopping_rounds=10, \n",
    "                    xgbclassifier__eval_set=[(preprocessor.transform(X_test), y_test)], xgbclassifier__verbose=False)\n",
    "\n",
    "    # Save train and test scores\n",
    "    train_accuracy= grid_search.score(X_train, y_train)\n",
    "    test_accuracy = grid_search.score(X_test, y_test)\n",
    "\n",
    "    # Save test sets\n",
    "    test_set_df = pd.DataFrame(data=np.column_stack((X_test, y_test)), columns=X.columns.tolist() + ['target_column'])\n",
    "    test_sets.append(test_set_df)\n",
    "\n",
    "    train_scores.append(train_accuracy)\n",
    "    test_scores.append(test_accuracy)\n",
    "    best_models.append(grid_search.best_estimator_)\n",
    "    res_xgb = grid_search.cv_results_\n",
    "\n",
    "    print(f\"Random State: {random_state},'Best Model': {grid_search.best_estimator_}, Train Score: {train_accuracy:.4f}, Test Score: {test_accuracy:.4f}\")\n",
    "\n",
    "# Save train and test scores\n",
    "scores_df = pd.DataFrame({'Random State': random_states, 'Best Model': best_models, 'Train Score': train_scores, 'Test Score': test_scores})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_test_score.loc[len(res_test_score)] = ['XGBoost', scores_df['Test Score'].mean(), scores_df['Test Score'].std()]\n",
    "res_test_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.6 model results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_scores = pd.read_csv('model_output/result_scores.csv')\n",
    "result_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_test_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1 Comparing with baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict all class as poisonous\n",
    "baseline_pred = np.ones(len(y_test))\n",
    "baseline_acc = accuracy_score(y_test, baseline_pred)\n",
    "\n",
    "print(f\"Baseline Accuracy: {baseline_acc:.5f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_mean = np.array(result_scores['mean_test']) \n",
    "test_std = np.array(result_scores['std_test']) \n",
    "labels = result_scores['Model']\n",
    "\n",
    "# Plot the mean values\n",
    "plt.figure(figsize=(10, 7))\n",
    "plt.bar(labels, test_mean, yerr=test_std, capsize=5, color='skyblue', alpha=0.7)\n",
    "\n",
    "plt.axhline(y=baseline_acc, linestyle='--', color='red', label='BaseLine Accuracy = {:.5f}'.format(baseline_acc))\n",
    "\n",
    "# Add labels and title\n",
    "plt.legend()\n",
    "plt.xlabel('Machine Learning Methods')\n",
    "plt.xticks(rotation=20)\n",
    "plt.ylabel('Test Accuracy Score')\n",
    "plt.title('Test Accuracy Score of each Machine Learning Method with Error Bars')\n",
    "\n",
    "# Add custom labels for error bars (optional)\n",
    "for i, (x, y, y_err) in enumerate(zip(labels, test_mean, test_std)):\n",
    "    plt.text(x, y-0.1, f'{y:.5f} $\\pm$ {y_err}', ha='center', va='bottom')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot error bar for non-linear models\n",
    "plt.errorbar(labels[1:], test_mean[1:], yerr=test_std[1:], fmt='o', capsize=5, label='Data with Error Bars')\n",
    "\n",
    "plt.xlabel('Machine Learning Methods')\n",
    "plt.ylabel('Test Accuracy Score')\n",
    "plt.title('Test Accuracy Score of Non-linear Methods with Error Bars')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Feature Importance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K Nearest Neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KNN\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.8, random_state=0)\n",
    "\n",
    "clf = Pipeline(steps=[('preprocessor', preprocessor)]) # for now we only preprocess \n",
    "                                                       # later on we will add train steps here\n",
    "clf.set_output(transform='pandas')\n",
    "\n",
    "X_train_prep = clf.fit_transform(X_train) # save for later use\n",
    "X_test_prep = clf.transform(X_test)\n",
    "print(X_train_prep.shape)\n",
    "\n",
    "feature_names = preprocessor.get_feature_names_out()\n",
    "\n",
    "# choose KNN as the final model\n",
    "final_model = KNeighborsClassifier(n_neighbors=3)\n",
    "final_model.fit(X_train_prep, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = final_model.predict(X_test_prep)\n",
    "print(f'test set accuracy score: {final_model.score(X_test_prep, y_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_test,y_pred)\n",
    "disp = ConfusionMatrixDisplay(cm,display_labels=['class 0', 'class 1'])\n",
    "fig, ax = plt.subplots(figsize=(6,4))\n",
    "plt.title('KNN - n_neighbors=3')\n",
    "disp.plot(ax=ax)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.8, random_state=0)\n",
    "\n",
    "clf = Pipeline(steps=[('preprocessor', preprocessor)]) # for now we only preprocess \n",
    "                                                       # later on we will add train steps here\n",
    "clf.set_output(transform='pandas')\n",
    "\n",
    "X_train_prep = clf.fit_transform(X_train) # save for later use\n",
    "X_test_prep = clf.transform(X_test)\n",
    "print(X_train_prep.shape)\n",
    "\n",
    "feature_names = preprocessor.get_feature_names_out()\n",
    "\n",
    "# choose KNN as the final model\n",
    "rf_model = RandomForestClassifier(max_depth=20, max_features=0.25, n_estimators=20)\n",
    "rf_model.fit(X_train_prep, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred =rf_model.predict(X_test_prep)\n",
    "print(f'test set accuracy score: {rf_model.score(X_test_prep, y_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_test,y_pred)\n",
    "disp = ConfusionMatrixDisplay(cm,display_labels=['class 0', 'class 1'])\n",
    "fig, ax = plt.subplots(figsize=(6,4))\n",
    "plt.title('Random forest -max_depth=20, max_features=0.25, n_estimators=20')\n",
    "disp.plot(ax=ax)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.1 Global feature importance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1.1 Using KNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Permutation Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n",
    "np.random.seed(0)\n",
    "\n",
    "ftr_names = X_test_prep.columns\n",
    "\n",
    "nr_runs = 3\n",
    "scores = np.zeros([len(ftr_names),nr_runs])\n",
    "\n",
    "test_score = final_model.score(X_test_prep, y_test)\n",
    "print('test score = ',test_score)\n",
    "print('test baseline = ', baseline_acc)\n",
    "# loop through the features\n",
    "for i in range(len(ftr_names)):\n",
    "    print('shuffling '+str(ftr_names[i]))\n",
    "    acc_scores = []\n",
    "    for j in range(nr_runs):\n",
    "        X_test_shuffled = X_test_prep.copy()\n",
    "        X_test_shuffled[ftr_names[i]] = np.random.permutation(X_test_prep[ftr_names[i]].values)\n",
    "        acc_scores.append(final_model.score(X_test_shuffled,y_test))\n",
    "    print('   shuffled test score:',np.around(np.mean(acc_scores),3),'+/-',np.around(np.std(acc_scores),3))\n",
    "    scores[i] = acc_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_indcs = np.argsort(np.abs(np.mean(scores,axis=1)-test_score))\n",
    "plt.rcParams.update({'font.size': 11})\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.boxplot(scores[sorted_indcs[-10:]].T,labels=ftr_names[sorted_indcs[-10:]],vert=False)\n",
    "plt.axvline(test_score,label='test score')\n",
    "plt.title(\"Permutation Importances (test set)\")\n",
    "plt.xlabel('score with perturbed feature')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SHAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(shap.kmeans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.initjs() # required for visualizations later on\n",
    "# create the explainer object with the random forest model\n",
    "#background_samples = shap.sample(X_train_prep, nsamples=120, random_state=42)\n",
    "background_samples = shap.kmeans(X_train_prep, k=20)\n",
    "explainer = shap.KernelExplainer(final_model.predict_proba, data=background_samples)\n",
    "# transform the test set\n",
    "X_test_transformed = X_test_prep\n",
    "print(np.shape(X_test_transformed))\n",
    "# calculate shap values on the first 1000 points in the test\n",
    "shap_values_knn = explainer.shap_values(X_test_transformed[:1000])\n",
    "print(np.shape(shap_values_knn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(np.sum(np.abs(shap_values_knn[1]),axis=0)+np.sum(np.abs(shap_values_knn[0]),axis=0)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap_summary = np.sum(np.abs(shap_values_knn[1]),axis=0)+np.sum(np.abs(shap_values_knn[0]),axis=0)\n",
    "indcs = np.argsort(shap_summary)\n",
    "shap_summary[indcs]\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.barh(feature_names[indcs[-10:]],shap_summary[indcs[-10:]])\n",
    "plt.title('SHAP value of 10 most important features')\n",
    "plt.xlabel('mean(|SHAP value|)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1.2 Using Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Permutation importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n",
    "np.random.seed(0)\n",
    "\n",
    "ftr_names = X_test_prep.columns\n",
    "\n",
    "nr_runs = 3\n",
    "scores = np.zeros([len(ftr_names),nr_runs])\n",
    "\n",
    "test_score = rf_model.score(X_test_prep, y_test)\n",
    "print('test score = ',test_score)\n",
    "print('test baseline = ', baseline_acc)\n",
    "# loop through the features\n",
    "for i in range(len(ftr_names)):\n",
    "    print('shuffling '+str(ftr_names[i]))\n",
    "    acc_scores = []\n",
    "    for j in range(nr_runs):\n",
    "        X_test_shuffled = X_test_prep.copy()\n",
    "        X_test_shuffled[ftr_names[i]] = np.random.permutation(X_test_prep[ftr_names[i]].values)\n",
    "        acc_scores.append(rf_model.score(X_test_shuffled,y_test))\n",
    "    print('   shuffled test score:',np.around(np.mean(acc_scores),3),'+/-',np.around(np.std(acc_scores),3))\n",
    "    scores[i] = acc_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_indcs = np.argsort(np.abs(np.mean(scores,axis=1)-test_score))\n",
    "plt.rcParams.update({'font.size': 11})\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.boxplot(scores[sorted_indcs[-10:]].T,labels=ftr_names[sorted_indcs[-10:]],vert=False)\n",
    "plt.axvline(test_score,label='test score')\n",
    "plt.title(\"Permutation Importances (test set)\")\n",
    "plt.xlabel('score with perturbed feature')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SHAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.initjs() # required for visualizations later on\n",
    "# create the explainer object with the random forest model\n",
    "explainer = shap.TreeExplainer(rf_model)\n",
    "# transform the test set\n",
    "X_test_transformed = X_test_prep\n",
    "print(np.shape(X_test_transformed))\n",
    "# calculate shap values on the first 1000 points in the test\n",
    "shap_values = explainer.shap_values(X_test_transformed[:1000])\n",
    "print(np.shape(shap_values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap_summary = np.sum(np.abs(shap_values[1]),axis=0)+np.sum(np.abs(shap_values[0]),axis=0)\n",
    "indcs = np.argsort(shap_summary)\n",
    "shap_summary[indcs]\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.barh(feature_names[indcs[-10:]],shap_summary[indcs[-10:]])\n",
    "plt.title('SHAP value of 10 most important features')\n",
    "plt.xlabel('mean(|SHAP value|)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MDI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importances = rf_model.feature_importances_\n",
    "std = np.std([tree.feature_importances_ for tree in rf_model.estimators_], axis=0)\n",
    "\n",
    "forest_importances = pd.DataFrame(pd.Series(importances, index=feature_names)).reset_index()\n",
    "rf_importance = pd.concat([forest_importances,pd.DataFrame(std)], axis=1)\n",
    "rf_importance.columns = ['feature', 'importance', 'std']\n",
    "rf_importance = rf_importance.sort_values('importance')\n",
    "rf_importance = rf_importance.set_index('feature')\n",
    "rf_importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10,8))\n",
    "\n",
    "rf_importance['importance'][-10:].plot.bar(yerr=rf_importance['std'][-10:], ax=ax)\n",
    "ax.set_title(\"Feature importances using MDI\")\n",
    "ax.set_ylabel(\"Mean decrease in impurity\")\n",
    "plt.xticks(rotation=30)\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2 Local feature importance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2.1 Using KNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SHAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 0\n",
    "shap.force_plot(explainer.expected_value[1], shap_values_knn[1][index,:], features = X_test_transformed.iloc[index,:],feature_names = feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 99\n",
    "shap.force_plot(explainer.expected_value[1], shap_values_knn[1][index,:], features = X_test_transformed.iloc[index,:],feature_names = feature_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LIME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lime import lime_tabular\n",
    "from lime.lime_text import LimeTextExplainer\n",
    "\n",
    "instance_to_explain = X_test_prep.iloc[0]\n",
    "\n",
    "# Replace 'feature_names' with the actual feature names from your dataset\n",
    "feature = [f\"Feature_{i}\" for i in range(X_train.shape[1])]\n",
    "\n",
    "# Create LIME explainer\n",
    "explainer = lime_tabular.LimeTabularExplainer(X_train_prep.to_numpy(), feature_names=list(feature_names))\n",
    "\n",
    "# Explain the prediction for the chosen instance\n",
    "explanation = explainer.explain_instance(instance_to_explain, final_model.predict_proba)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explanation.show_in_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data1030",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
